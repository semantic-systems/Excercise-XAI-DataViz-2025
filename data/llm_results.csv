Unnamed: 0,deepseek-ai/DeepSeek-R1-Distill-Llama-70B,mistralai/Mistral-Large-Instruct-2411,meta-llama/Llama-3.3-70B-Instruct,google/gemma-2-27b-it
bbh.acc_norm,0.562229,0.670023,0.691547,0.64225
bbh_boolean_expressions.acc_norm,0.868,0.912,0.916,0.872
bbh_causal_judgement.acc_norm,0.518717,0.55615,0.679144,0.609626
bbh_date_understanding.acc_norm,0.656,0.784,0.704,0.656
bbh_disambiguation_qa.acc_norm,0.396,0.752,0.664,0.736
bbh_formal_fallacies.acc_norm,0.652,0.696,0.82,0.628
bbh_geometric_shapes.acc_norm,0.556,0.432,0.352,0.456
bbh_hyperbaton.acc_norm,0.56,0.844,0.708,0.788
bbh_logical_deduction_five_objects.acc_norm,0.504,0.676,0.656,0.592
bbh_logical_deduction_seven_objects.acc_norm,0.496,0.644,0.624,0.568
bbh_logical_deduction_three_objects.acc_norm,0.548,0.94,0.92,0.836
bbh_movie_recommendation.acc_norm,0.728,0.824,0.784,0.696
bbh_navigate.acc_norm,0.42,0.5,0.696,0.696
bbh_object_counting.acc_norm,0.612,0.196,0.58,0.356
bbh_penguins_in_a_table.acc_norm,0.59589,0.856164,0.643836,0.705479
bbh_reasoning_about_colored_objects.acc_norm,0.7,0.908,0.892,0.816
bbh_ruin_names.acc_norm,0.864,0.884,0.852,0.816
bbh_salient_translation_error_detection.acc_norm,0.616,0.684,0.672,0.6
bbh_snarks.acc_norm,0.657303,0.88764,0.803371,0.820225
bbh_sports_understanding.acc_norm,0.484,0.78,0.952,0.836
bbh_temporal_sequences.acc_norm,0.888,0.996,1.0,0.896
bbh_tracking_shuffled_objects_five_objects.acc_norm,0.172,0.284,0.324,0.288
bbh_tracking_shuffled_objects_seven_objects.acc_norm,0.148,0.336,0.26,0.292
bbh_tracking_shuffled_objects_three_objects.acc_norm,0.332,0.336,0.36,0.384
bbh_web_of_lies.acc_norm,0.552,0.484,0.744,0.54
gpqa.acc_norm,0.265101,0.437081,0.328859,0.375
gpqa_diamond.acc_norm,0.292929,0.388889,0.292929,0.393939
gpqa_extended.acc_norm,0.267399,0.441392,0.342491,0.371795
gpqa_main.acc_norm,0.25,0.453125,0.328125,0.370536
ifeval.prompt_level_strict_acc,0.365989,0.809612,0.879852,0.763401
ifeval.inst_level_strict_acc,0.501199,0.870504,0.919664,0.832134
ifeval.prompt_level_loose_acc,0.373383,0.850277,0.892791,0.787431
ifeval.inst_level_loose_acc,0.51199,0.898082,0.928058,0.85012
math_hard.exact_match,0.307402,0.495468,0.483384,0.238671
math_algebra_hard.exact_match,0.501629,0.758958,0.775244,0.439739
math_counting_and_prob_hard.exact_match,0.317073,0.479675,0.536585,0.154472
math_geometry_hard.exact_match,0.219697,0.310606,0.310606,0.090909
math_intermediate_algebra_hard.exact_match,0.042857,0.232143,0.167857,0.046429
math_num_theory_hard.exact_match,0.324675,0.519481,0.467532,0.220779
math_prealgebra_hard.exact_match,0.595855,0.735751,0.740933,0.487047
math_precalculus_hard.exact_match,0.059259,0.266667,0.244444,0.066667
mmlu_pro.acc,0.474817,0.556184,0.533162,0.445146
musr.acc_norm,0.43254,0.452381,0.444444,0.402116
musr_murder_mysteries.acc_norm,0.5,0.572,0.536,0.544
musr_object_placements.acc_norm,0.222656,0.25,0.234375,0.253906
musr_team_allocation.acc_norm,0.58,0.54,0.568,0.412
